### mechanism:
[self-attention](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)
-  most papers still implement the original *scaled-dot product attention mechanism* discussed in this paper since it usually results in superior accuracy and because self-attention is rarely a computational bottleneck for most companies training large-scale transformers
-  