{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "03b8969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "96ee6de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: torch\n",
      "Version: 2.1.1\n",
      "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
      "Home-page: https://pytorch.org/\n",
      "Author: PyTorch Team\n",
      "Author-email: packages@pytorch.org\n",
      "License: BSD-3\n",
      "Location: C:\\Users\\Steve\\anaconda3\\Lib\\site-packages\n",
      "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
      "Required-by: monai-weekly\n"
     ]
    }
   ],
   "source": [
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f778b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider the sentence \"Life is short, eat dessert first\"\n",
    "sentence = 'Life is short, eat dessert first'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a6d45e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Life': 0, 'dessert': 1, 'eat': 2, 'first': 3, 'is': 4, 'short': 5}\n"
     ]
    }
   ],
   "source": [
    "dc = {s:i for i,s in enumerate(sorted(sentence.replace(',', '').split()))}\n",
    "print(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2e9dfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 4, 5, 2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "sentence_int = torch.tensor([dc[s] for s in sentence.replace(',', '').split()])\n",
    "print(sentence_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "370a4e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, using the integer-vector representation of the input sentence, we can use an embedding \n",
    "# layer to encode the inputs into a real-vector embedding. Here, we will use a 16-dimensional \n",
    "# embedding such that each input word is represented by a 16-dimensional vector. Since the sentence \n",
    "# consists of 6 words, this will result in a 6×16-dimensional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "043e0d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(6, 16)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
      "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
      "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
      "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
      "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
      "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
      "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
      "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293],\n",
      "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
      "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
      "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
      "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729]],\n",
      "       requires_grad=True)\n",
      "torch.Size([6, 16])\n",
      "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  0.3486,  0.6603, -0.2196, -0.3792,\n",
      "          0.7671, -1.1925,  0.6984, -1.4097,  0.1794,  1.8951,  0.4954,  0.2692],\n",
      "        [ 0.5146,  0.9938, -0.2587, -1.0826, -0.0444,  1.6236, -2.3229,  1.0878,\n",
      "          0.6716,  0.6933, -0.9487, -0.0765, -0.1526,  0.1167,  0.4403, -1.4465],\n",
      "        [ 0.2553, -0.5496,  1.0042,  0.8272, -0.3948,  0.4892, -0.2168, -1.7472,\n",
      "         -1.6025, -1.0764,  0.9031, -0.7218, -0.5951, -0.7112,  0.6230, -1.3729],\n",
      "        [-1.3250,  0.1784, -2.1338,  1.0524, -0.3885, -0.9343, -0.4991, -1.0867,\n",
      "          0.8805,  1.5542,  0.6266, -0.1755,  0.0983, -0.0935,  0.2662, -0.5850],\n",
      "        [-0.0770, -1.0205, -0.1690,  0.9178,  1.5810,  1.3010,  1.2753, -0.2010,\n",
      "          0.4965, -1.5723,  0.9666, -1.1481, -1.1589,  0.3255, -0.6315, -2.8400],\n",
      "        [ 0.8768,  1.6221, -1.4779,  1.1331, -1.2203,  1.3139,  1.0533,  0.1388,\n",
      "          2.2473, -0.8036, -0.2808,  0.7697, -0.6596, -0.7979,  0.1838,  0.2293]])\n",
      "torch.Size([6, 16])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) # ensure the reproductibility with the same random number production sequence\n",
    "embed = torch.nn.Embedding(6, 16) # 6 means can represent 0 ~ 5\n",
    "embedded_sentence = embed(sentence_int).detach()\n",
    "\n",
    "print(embed) \n",
    "print(embed.weight) # stack the 6 dim 16 tensor together\n",
    "print(embed.weight.shape)\n",
    "print(embedded_sentence) # change each of input [0, 4, 5, 2, 1, 3]  to a dim 16 tensor. \n",
    "print(embedded_sentence.shape)\n",
    "print(embedded_sentence[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e82ebac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-attention utilizes three weight matrices, referred to as Wq, Wk,and Wv\n",
    "# which are adjusted as model parameters during training. These matrices serve \n",
    "# to project the inputs into query, key, and value components of the sequence, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58757af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query sequence: q(i)=Wq*x(i) for i∈[1,T]\n",
    "# Key sequence: k(i)=Wk*x(i) for i∈[1,T]\n",
    "# Value sequence: v(i)=Wv*x(i) for  i∈[1,T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "452fe6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:  16\n",
      "W_query shape:  torch.Size([24, 16])  W_key shape:  torch.Size([24, 16]) W_value shape:  torch.Size([28, 16])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "d = embedded_sentence.shape[1]\n",
    "print(\"d: \", d)\n",
    "\n",
    "d_q, d_k, d_v = 24, 24, 28\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_q, d))\n",
    "W_key = torch.nn.Parameter(torch.rand(d_k, d))\n",
    "W_value = torch.nn.Parameter(torch.rand(d_v, d))\n",
    "print (\"W_query shape: \", W_query.shape, \" W_key shape: \", W_key.shape, \"W_value shape: \", W_value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9ed9d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let’s suppose we are interested in computing the attention-vector for the second input element – the second input element acts as the query here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cd94c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_w shape torch.Size([16])\n",
      "torch.Size([24])\n",
      "torch.Size([24])\n",
      "torch.Size([28])\n"
     ]
    }
   ],
   "source": [
    "x_2 = embedded_sentence[1]\n",
    "print(\"x_w shape\", x_2.shape)\n",
    "query_2 = W_query.matmul(x_2)\n",
    "key_2 = W_key.matmul(x_2)\n",
    "value_2 = W_value.matmul(x_2)\n",
    "\n",
    "print(query_2.shape)\n",
    "print(key_2.shape)\n",
    "print(value_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b3cb595e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([3])\n",
      "tensor([2.1710, 2.6036, 4.5133], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "A = torch.tensor([1,2,3], dtype=float)\n",
    "print(A.shape)\n",
    "print(A.T.shape)\n",
    "# A = A.T\n",
    "B = torch.rand(3,3, dtype=float)\n",
    "print(B.matmul(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8067156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 24])\n",
      "values.shape: torch.Size([6, 28])\n"
     ]
    }
   ],
   "source": [
    "keys = W_key.matmul(embedded_sentence.T).T\n",
    "values = W_value.matmul(embedded_sentence.T).T\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb7c7482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.1466, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "omega_24 = query_2.dot(keys[4])\n",
    "print(omega_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "eeb696ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.5808, -7.6597,  3.2558,  1.0395, 11.1466, -0.4800],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "omega_2 = query_2.matmul(keys.T)\n",
    "print(omega_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7f1272ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The subsequent step in self-attention is to normalize the unnormalized attention weights, \n",
    "# ω, to obtain the normalized attention weights, α, by applying the softmax function. \n",
    "# Additionally, 1/√dk is used to scale ω before normalizing it through the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2e02b15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2912, 0.0106, 0.0982, 0.0625, 0.4917, 0.0458],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)\n",
    "print(attention_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2240601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, the last step is to compute the context vector z(2)\n",
    "# , which is an attention-weighted version of our original query input x(2)\n",
    "# , including all the other input elements as its context via the attention weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3b6ae153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28])\n",
      "tensor([-1.5993,  0.0156,  1.2670,  0.0032, -0.6460, -1.1407, -0.4908, -1.4632,\n",
      "         0.4747,  1.1926,  0.4506, -0.7110,  0.0602,  0.7125, -0.1628, -2.0184,\n",
      "         0.3838, -2.1188, -0.8136, -1.5694,  0.7934, -0.2911, -1.3640, -0.2366,\n",
      "        -0.9564, -0.5265,  0.0624,  1.7084], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "context_vector_2 = attention_weights_2.matmul(values)\n",
    "\n",
    "print(context_vector_2.shape)\n",
    "print(context_vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c00a7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this output vector has more dimensions (dv=28) than the original input vector (d=16) \n",
    "# since we specified dv>d earlier; however, the embedding size choice is arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0d134b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention (3 heads as example)\n",
    "h = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8055627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To illustrate this in code, suppose we have 3 attention heads, so we now extend the d′×d\n",
    "# dimensional weight matrices so 3×d′×d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f24dbd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_W_query shape:  torch.Size([3, 24, 16])\n",
      "multihead_W_key shape:  torch.Size([3, 24, 16])\n",
      "multihead_W_value shape:  torch.Size([3, 28, 16])\n"
     ]
    }
   ],
   "source": [
    "multihead_W_query = torch.nn.Parameter(torch.rand(h, d_q, d))\n",
    "multihead_W_key = torch.nn.Parameter(torch.rand(h, d_k, d))\n",
    "multihead_W_value = torch.nn.Parameter(torch.rand(h, d_v, d))\n",
    "print(\"multihead_W_query shape: \", multihead_W_query.shape)\n",
    "print(\"multihead_W_key shape: \", multihead_W_key.shape)\n",
    "print(\"multihead_W_value shape: \", multihead_W_value.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1ff9e208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 24])\n"
     ]
    }
   ],
   "source": [
    "multihead_query_2 = multihead_W_query.matmul(x_2)\n",
    "print(multihead_query_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e1aef2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 6])\n",
      "tensor([[[ 0.3374,  0.5146,  0.2553, -1.3250, -0.0770,  0.8768],\n",
      "         [-0.1778,  0.9938, -0.5496,  0.1784, -1.0205,  1.6221],\n",
      "         [-0.3035, -0.2587,  1.0042, -2.1338, -0.1690, -1.4779],\n",
      "         [-0.5880, -1.0826,  0.8272,  1.0524,  0.9178,  1.1331],\n",
      "         [ 0.3486, -0.0444, -0.3948, -0.3885,  1.5810, -1.2203],\n",
      "         [ 0.6603,  1.6236,  0.4892, -0.9343,  1.3010,  1.3139],\n",
      "         [-0.2196, -2.3229, -0.2168, -0.4991,  1.2753,  1.0533],\n",
      "         [-0.3792,  1.0878, -1.7472, -1.0867, -0.2010,  0.1388],\n",
      "         [ 0.7671,  0.6716, -1.6025,  0.8805,  0.4965,  2.2473],\n",
      "         [-1.1925,  0.6933, -1.0764,  1.5542, -1.5723, -0.8036],\n",
      "         [ 0.6984, -0.9487,  0.9031,  0.6266,  0.9666, -0.2808],\n",
      "         [-1.4097, -0.0765, -0.7218, -0.1755, -1.1481,  0.7697],\n",
      "         [ 0.1794, -0.1526, -0.5951,  0.0983, -1.1589, -0.6596],\n",
      "         [ 1.8951,  0.1167, -0.7112, -0.0935,  0.3255, -0.7979],\n",
      "         [ 0.4954,  0.4403,  0.6230,  0.2662, -0.6315,  0.1838],\n",
      "         [ 0.2692, -1.4465, -1.3729, -0.5850, -2.8400,  0.2293]],\n",
      "\n",
      "        [[ 0.3374,  0.5146,  0.2553, -1.3250, -0.0770,  0.8768],\n",
      "         [-0.1778,  0.9938, -0.5496,  0.1784, -1.0205,  1.6221],\n",
      "         [-0.3035, -0.2587,  1.0042, -2.1338, -0.1690, -1.4779],\n",
      "         [-0.5880, -1.0826,  0.8272,  1.0524,  0.9178,  1.1331],\n",
      "         [ 0.3486, -0.0444, -0.3948, -0.3885,  1.5810, -1.2203],\n",
      "         [ 0.6603,  1.6236,  0.4892, -0.9343,  1.3010,  1.3139],\n",
      "         [-0.2196, -2.3229, -0.2168, -0.4991,  1.2753,  1.0533],\n",
      "         [-0.3792,  1.0878, -1.7472, -1.0867, -0.2010,  0.1388],\n",
      "         [ 0.7671,  0.6716, -1.6025,  0.8805,  0.4965,  2.2473],\n",
      "         [-1.1925,  0.6933, -1.0764,  1.5542, -1.5723, -0.8036],\n",
      "         [ 0.6984, -0.9487,  0.9031,  0.6266,  0.9666, -0.2808],\n",
      "         [-1.4097, -0.0765, -0.7218, -0.1755, -1.1481,  0.7697],\n",
      "         [ 0.1794, -0.1526, -0.5951,  0.0983, -1.1589, -0.6596],\n",
      "         [ 1.8951,  0.1167, -0.7112, -0.0935,  0.3255, -0.7979],\n",
      "         [ 0.4954,  0.4403,  0.6230,  0.2662, -0.6315,  0.1838],\n",
      "         [ 0.2692, -1.4465, -1.3729, -0.5850, -2.8400,  0.2293]],\n",
      "\n",
      "        [[ 0.3374,  0.5146,  0.2553, -1.3250, -0.0770,  0.8768],\n",
      "         [-0.1778,  0.9938, -0.5496,  0.1784, -1.0205,  1.6221],\n",
      "         [-0.3035, -0.2587,  1.0042, -2.1338, -0.1690, -1.4779],\n",
      "         [-0.5880, -1.0826,  0.8272,  1.0524,  0.9178,  1.1331],\n",
      "         [ 0.3486, -0.0444, -0.3948, -0.3885,  1.5810, -1.2203],\n",
      "         [ 0.6603,  1.6236,  0.4892, -0.9343,  1.3010,  1.3139],\n",
      "         [-0.2196, -2.3229, -0.2168, -0.4991,  1.2753,  1.0533],\n",
      "         [-0.3792,  1.0878, -1.7472, -1.0867, -0.2010,  0.1388],\n",
      "         [ 0.7671,  0.6716, -1.6025,  0.8805,  0.4965,  2.2473],\n",
      "         [-1.1925,  0.6933, -1.0764,  1.5542, -1.5723, -0.8036],\n",
      "         [ 0.6984, -0.9487,  0.9031,  0.6266,  0.9666, -0.2808],\n",
      "         [-1.4097, -0.0765, -0.7218, -0.1755, -1.1481,  0.7697],\n",
      "         [ 0.1794, -0.1526, -0.5951,  0.0983, -1.1589, -0.6596],\n",
      "         [ 1.8951,  0.1167, -0.7112, -0.0935,  0.3255, -0.7979],\n",
      "         [ 0.4954,  0.4403,  0.6230,  0.2662, -0.6315,  0.1838],\n",
      "         [ 0.2692, -1.4465, -1.3729, -0.5850, -2.8400,  0.2293]]])\n"
     ]
    }
   ],
   "source": [
    "stacked_inputs = embedded_sentence.T.repeat(3, 1, 1)\n",
    "print(stacked_inputs.shape)\n",
    "# print(stacked_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d55f2bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 24, 6])\n",
      "multihead_values.shape: torch.Size([3, 28, 6])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = torch.bmm(multihead_W_key, stacked_inputs) # bmm batch matrix multiplication\n",
    "multihead_values = torch.bmm(multihead_W_value, stacked_inputs)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "41bac0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multihead_keys.shape: torch.Size([3, 6, 24])\n",
      "multihead_values.shape: torch.Size([3, 6, 28])\n"
     ]
    }
   ],
   "source": [
    "multihead_keys = multihead_keys.permute(0, 2, 1)\n",
    "multihead_values = multihead_values.permute(0, 2, 1)\n",
    "print(\"multihead_keys.shape:\", multihead_keys.shape)\n",
    "print(\"multihead_values.shape:\", multihead_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2284641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we follow the same steps as previously to compute the unscaled \n",
    "# attention weights ω and attention weights α, followed by the scaled\n",
    "# -softmax computation to obtain an h×dv (here: 3×dv) dimensional context \n",
    "# vector z for the input element x(2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6febf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
